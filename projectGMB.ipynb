{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abdulhafizsuherman/Python_GMB-Tea-Leaves/blob/master/projectGMB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "ZQa9BZFd_MCL",
        "outputId": "ecadad6e-69a1-4777-978c-ac6a7cfc5cfd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "h2p9hCHP8qcl",
        "outputId": "d6b45e47-498a-46d6-dcba-bc94824ed5d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Kuliah/Semester 8/TA\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/My Drive/Kuliah/Semester 8/TA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Uh_SZcs2v9e"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from imutils import paths\n",
        "from keras.layers import AveragePooling2D\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "6oGka-4H_iSR",
        "outputId": "9ed040e3-27fd-4ed2-a7d7-c1da4575a9a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n"
          ]
        }
      ],
      "source": [
        "# TODO grab all image paths\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = paths.list_images(\"proses2_datasetbaru(channelGreen)\")\n",
        "data = []\n",
        "labels = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "1QUH4YfGuS-I",
        "outputId": "31ff1ec5-0247-49c7-cd4d-be1106096c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LabelBinarizer(neg_label=0, pos_label=1, sparse_output=False)\n",
            "[[0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]\n",
            " [0 0 0 ... 0 0 1]]\n"
          ]
        }
      ],
      "source": [
        "for imagePath in imagePaths:\n",
        "    image = Image.open(imagePath)\n",
        "    image = np.array(image.resize((192, 192))) / 255.0\n",
        "    image = np.expand_dims(image, axis=-1)  # penambahan dimensi array untuk proses 1 kanal\n",
        "    data.append(image)\n",
        "\n",
        "    label = imagePath.split(os.path.sep)[-2]\n",
        "    labels.append(label)\n",
        "\n",
        "# TODO encode the labels\n",
        "lb = LabelBinarizer()\n",
        "labels = lb.fit_transform(labels)\n",
        "\n",
        "print(lb)\n",
        "print(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63iYxz7v5qpI"
      },
      "outputs": [],
      "source": [
        "# TODO perform a training and testing split\n",
        "''' test images : testX\n",
        "    test labels : testY\n",
        "    train images : trainX\n",
        "    train labels : trainY'''\n",
        "\n",
        "(trainX, testX, trainY, testY) = train_test_split(np.array(data),\n",
        "                                                  np.array(labels), test_size=0.25)\n",
        "print(\"[INFO] Shape train and test array...\")\n",
        "print(\"ukuran train X: \", trainX.shape)\n",
        "print(\"ukuran test X: \", testX.shape)\n",
        "print(\"ukuran train Y: \", trainY.shape)\n",
        "print(\"ukuran test Y: \", testY.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xDnWEv55xD6"
      },
      "outputs": [],
      "source": [
        "# TODO Initialising the CNN with Architecture LeNet-5\n",
        "\n",
        "class Lenet5:\n",
        "    def __init__(self, imgRows, imgCols, numChannels):\n",
        "        self.numChannels = numChannels\n",
        "        self.imgRows = imgRows\n",
        "        self.imgCols = imgCols\n",
        "\n",
        "    def architecture(self):\n",
        "        # initialize the model\n",
        "        classifier = Sequential()\n",
        "        inputShape = (self.imgRows, self.imgCols, self.numChannels)\n",
        "\n",
        "        kernel_size = (5, 5)\n",
        "        strides = (2, 2)\n",
        "        pool_size = (2, 2)\n",
        "        # Step 1 - Convolution(C1)\n",
        "        classifier.add(Conv2D(6, kernel_size=kernel_size, strides=strides,\n",
        "                              input_shape=inputShape, activation='relu'))\n",
        "\n",
        "        # Step 2 - Average Pooling(S2)\n",
        "        classifier.add(AveragePooling2D(pool_size=pool_size, strides=strides))\n",
        "\n",
        "        # Step 3 - Convolution(C3)\n",
        "        classifier.add(Conv2D(16, kernel_size=kernel_size, activation='relu'))\n",
        "\n",
        "        # step 4 - Average Pooling(S4)\n",
        "        classifier.add(AveragePooling2D(pool_size=strides, strides=strides))\n",
        "\n",
        "        # Step 5 - Flattening\n",
        "        classifier.add(Flatten())\n",
        "\n",
        "        # Step 4 - Full connection\n",
        "        classifier.add(Dense(units=120, activation='relu'))\n",
        "        classifier.add(Dense(units=84, activation='relu'))\n",
        "        classifier.add(Dense(units=11, activation='softmax'))\n",
        "        return classifier\n",
        "\n",
        "\n",
        "model = Lenet5(192, 192, 1).architecture()  # define method\n",
        "# TODO train the model using the Adam optimizer / compiling CNN\n",
        "\n",
        "print(\"[INFO] training network...\")\n",
        "opt = Adam(lr=0.001, decay=1e-2 / 50)  \n",
        "# opt = Adam(lr=0.001)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "\"\"\" Since we are using the categorical cross-entropy loss function, we need to apply the to_categorical  function \n",
        "which converts our labels from integers to a vector, where each vector ranges from [0, classes] . This function \n",
        "generates a vector for each class label, where the index of the correct label is set to 1 and all other entries are \n",
        "set to 0. \"\"\"\n",
        "\n",
        "history = model.fit(trainX, trainY, validation_data=(testX, testY), epochs=100, batch_size=32)\n",
        "\n",
        "# ======================================================================================================================\n",
        "\n",
        "# TODO evaluate the network\n",
        "\n",
        "print(\"[INFO] evaluating network using classification report...\")\n",
        "\n",
        "y_pred = model.predict(testX, batch_size=32)\n",
        "testY_argm = testY.argmax(axis=1)\n",
        "y_pred1_argm = y_pred.argmax(axis=1)\n",
        "\n",
        "print('Accuracy Score :', accuracy_score(testY_argm, y_pred1_argm))\n",
        "\n",
        "clas_report = classification_report(testY_argm, y_pred1_argm, target_names=lb.classes_)\n",
        "print(\"==Classification Report==\")\n",
        "print(clas_report)\n",
        "\n",
        "\n",
        "# ======================================================================================================================\n",
        "\n",
        "# TODO confusion matrix\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap='Blues'):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "\n",
        "cnf_matrix = confusion_matrix(testY_argm, y_pred1_argm)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "print(\"==Confusion Matrix==\")\n",
        "print(cnf_matrix)\n",
        "\n",
        "# Plot non/normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=lb.classes_, normalize=True)\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=lb.classes_, normalize=False)\n",
        "plt.show()\n",
        "\n",
        "# TODO plotting Value accuracy\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure()\n",
        "plt.style.use('ggplot')\n",
        "plt.plot(history.history['acc'], 'b-')\n",
        "plt.plot(history.history['val_acc'], 'r-')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train Images', 'Test Images'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.figure()\n",
        "plt.style.use('ggplot')\n",
        "plt.plot(history.history['loss'], 'b-o')\n",
        "plt.plot(history.history['val_loss'], 'r-v')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train Images', 'Test Images'], loc='upper left')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP5GPyhQ0XY8wgfTjp+OiBY",
      "collapsed_sections": [],
      "include_colab_link": true,
      "mount_file_id": "1YOqm_LifhuEVQh6omggvRpZbRsjplGEN",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.7.9 64-bit (microsoft store)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "4e480298e253edaa1ae6455593be9c2697fa9e8c89a2bbc7e930233b8caf9833"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
